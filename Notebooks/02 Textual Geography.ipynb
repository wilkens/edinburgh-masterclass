{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Textual geography \n",
    "\n",
    "This session will walk through some more advanced text processing and data wrangling to produce a map of the locations mentioned in our corpus. Topics covered include:\n",
    "\n",
    "* Named entity recognition using Stanford's CRF-NER package.\n",
    "* Geolocation using Google's mapping APIs.\n",
    "* Cartographic visualization, both static (for print publication) and interactive (for online use).\n",
    "\n",
    "## Named entity recognition\n",
    "\n",
    "There are several approaches to identifying the places used in a piece of text. We could rely on a dictionary or gazetteer, which would tell us that Edinburgh is a city in Scotland, but would also tell us that Charlotte BrÃ¶nte is a city in the United States.\n",
    "\n",
    "We'll instead use statistical machine learning methods. While we *will* get an intro to machine learning this afternoon, for now we'll rely on the [implementation by the Stanford NLP group](http://nlp.stanford.edu/software/CRF-NER.html).\n",
    "\n",
    "This is a Java package. It's possible -- with a lot of work -- to invoke Java code from Python. But there's no point in this case; it's much easier to invoke the NER package from the command line and to read in the plain text output.\n",
    "\n",
    "Note that there do exist NER, POS, and other NLP packages for Python. NLTK -- the Natural Language Tool Kit, which we met in the last notebook when we used it for corpus processing -- is one of the most diverse and well conceived. But it's not optimized for speed and isn't notably accurate compared to more production-oriented offerings. \n",
    "\n",
    "There's a guide to using the NER package on Stanford's site. Here's the short version, for reference:\n",
    "\n",
    "```\n",
    "java -mx1g -cp \"*:lib/*\" edu.stanford.nlp.ie.crf.CRFClassifier\n",
    "-loadClassifier classifiers/english.all.3class.distsim.crf.ser.gz \n",
    "-outputFormat tabbedEntities \n",
    "-textFile file.txt > file.tsv\n",
    "```\n",
    "\n",
    "This runs the English-language classifier over a single text file. Note that we need to have a classifier trained for the language of the text we're processing. Stanford has trained models for English, Spanish, German, Chinese, and other major languages, but they're lacking French and a great many other languages.\n",
    "\n",
    "The classidier's output follows a tabbed format that looks like this:\n",
    "\n",
    "```\n",
    "                Why did the poor poet of\n",
    "Tennessee       LOCATION        , upon suddenly receiving two handfuls of silver , deliberate whether to buy him a coat , which he sadly needed , or invest his money in a pedestrian trip to\n",
    "Rockaway Beach  LOCATION        ?\n",
    "```\n",
    "\n",
    "The thing to notice is that every line begins with two tabs, but only the text in front of the first tab is a recognized entity. The text in front of the second tab, then, indicates the type of entity: PERSON, ORGANIZATION, or LOCATION. Any text following the second tab is body text, presumed not to contain any named entities.\n",
    "\n",
    "So let's recreate our corpus, then read in the tagged files and get a list of the locations used in the corpus.\n",
    "\n",
    "### Recreate the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nation</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>gender</th>\n",
       "      <th>wordcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A-Cather-Antonia-1918-F.txt</th>\n",
       "      <td>A</td>\n",
       "      <td>Cather</td>\n",
       "      <td>Antonia</td>\n",
       "      <td>1918</td>\n",
       "      <td>F</td>\n",
       "      <td>97574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-Chesnutt-Marrow-1901-M.txt</th>\n",
       "      <td>A</td>\n",
       "      <td>Chesnutt</td>\n",
       "      <td>Marrow</td>\n",
       "      <td>1901</td>\n",
       "      <td>M</td>\n",
       "      <td>110288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-Crane-Maggie-1893-M.txt</th>\n",
       "      <td>A</td>\n",
       "      <td>Crane</td>\n",
       "      <td>Maggie</td>\n",
       "      <td>1893</td>\n",
       "      <td>M</td>\n",
       "      <td>28628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-Davis-Life_Iron_mills-1861-F.txt</th>\n",
       "      <td>A</td>\n",
       "      <td>Davis</td>\n",
       "      <td>Life Iron mills</td>\n",
       "      <td>1861</td>\n",
       "      <td>F</td>\n",
       "      <td>18789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-Dreiser-Sister_Carrie-1900-M.txt</th>\n",
       "      <td>A</td>\n",
       "      <td>Dreiser</td>\n",
       "      <td>Sister Carrie</td>\n",
       "      <td>1900</td>\n",
       "      <td>M</td>\n",
       "      <td>194062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   nation    author            title  pubdate  \\\n",
       "A-Cather-Antonia-1918-F.txt             A    Cather          Antonia     1918   \n",
       "A-Chesnutt-Marrow-1901-M.txt            A  Chesnutt           Marrow     1901   \n",
       "A-Crane-Maggie-1893-M.txt               A     Crane           Maggie     1893   \n",
       "A-Davis-Life_Iron_mills-1861-F.txt      A     Davis  Life Iron mills     1861   \n",
       "A-Dreiser-Sister_Carrie-1900-M.txt      A   Dreiser    Sister Carrie     1900   \n",
       "\n",
       "                                   gender  wordcount  \n",
       "A-Cather-Antonia-1918-F.txt             F      97574  \n",
       "A-Chesnutt-Marrow-1901-M.txt            M     110288  \n",
       "A-Crane-Maggie-1893-M.txt               M      28628  \n",
       "A-Davis-Life_Iron_mills-1861-F.txt      F      18789  \n",
       "A-Dreiser-Sister_Carrie-1900-M.txt      M     194062  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "text_dir = '../Data/Texts/'\n",
    "corpus = PlaintextCorpusReader(text_dir, '.*\\.txt')\n",
    "\n",
    "# A function to turn fileids into a table of metadata\n",
    "def parse_fileids(fileids):\n",
    "    '''Takes a list of file names formatted like A-Cather-Antonia-1918-F.txt.\n",
    "       Returns a pandas dataframe of derived metadata.'''\n",
    "    import pandas as pd\n",
    "    meta = {}\n",
    "    for fileid in fileids:\n",
    "        file = fileid.strip('.txt') # Get rid of file suffix\n",
    "        fields = file.split('-') # Split on dashes\n",
    "        fields[2] = fields[2].replace('_', ' ') # Remove underscore from titles\n",
    "        fields[3] = int(fields[3])\n",
    "        meta[fileid] = fields\n",
    "    metadata = pd.DataFrame.from_dict(meta, orient='index') # Build dataframe\n",
    "    metadata.columns = ['nation', 'author', 'title', 'pubdate', 'gender'] # Col names\n",
    "    return metadata.sort_index() # Note we need to sort b/c datframe built from dictionary\n",
    "\n",
    "def collect_stats(corpus):\n",
    "    '''Takes an NLTK corpus as input. \n",
    "       Returns a pandas dataframe of stats indexed to fileid.'''\n",
    "    import nltk\n",
    "    import pandas as pd\n",
    "    stats = {}\n",
    "    for fileid in corpus.fileids():\n",
    "        word_count = len(corpus.words(fileid))\n",
    "        stats[fileid] = {'wordcount':word_count}\n",
    "    statistics = pd.DataFrame.from_dict(stats, orient='index')\n",
    "    return statistics.sort_index()\n",
    "\n",
    "books = parse_fileids(corpus.fileids())\n",
    "stats = collect_stats(corpus)\n",
    "books = books.join(stats)\n",
    "books.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and parse NER-tagged files\n",
    "\n",
    "The taged NER files are in the `..Data/NER/` directory. We need a function that will parse each one and return just the locations for further processing. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [carto]",
   "language": "python",
   "name": "Python [carto]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
